{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9714f006-b7ca-4edf-9f4f-959a95dd102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af7b466-24f7-401f-b2e7-1b62190ba133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from Levenshtein import seqratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab51e46-9705-4238-86de-9eaea44a2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_input_and_gen_ids(input_ids, generated_ids, pad_id=0, eos_id=1, idx_a=32000, idx_b=32099):\n",
    "    new_input_ids = []\n",
    "    for k in range(len(input_ids)):\n",
    "        inp_ids = input_ids[k]\n",
    "        inp_len = (inp_ids != pad_id).sum().item()\n",
    "        gen_ids = generated_ids[k]\n",
    "        gen_len = (gen_ids != pad_id).sum().item()\n",
    "        z_x = ~((inp_ids >= idx_a) & (inp_ids <= idx_b))\n",
    "        z_x = z_x & (inp_ids != pad_id) & (inp_ids != eos_id)\n",
    "        z_x = z_x.long().tolist()\n",
    "        z_y = ((gen_ids >= idx_a) & (gen_ids <= idx_b))\n",
    "        z_y = z_y & (gen_ids != pad_id) & (gen_ids != eos_id)\n",
    "        z_y = z_y.long().tolist()\n",
    "        i, j = 0, 0\n",
    "        new_inp = []\n",
    "        while j < gen_len:\n",
    "            if z_y[j] == 1:\n",
    "                while z_x[i] == 1 and i < inp_len:\n",
    "                    new_inp.append(inp_ids[i].item())\n",
    "                    i += 1\n",
    "                j += 1\n",
    "                i += 1\n",
    "                if i >= inp_len:\n",
    "                    break\n",
    "            else:\n",
    "                new_inp.append(gen_ids[j].item())\n",
    "                j += 1\n",
    "        if i < inp_len:\n",
    "            new_inp.extend(inp_ids[i:inp_len])\n",
    "        if new_inp[-1] != 1 and new_inp[-1] != 0:\n",
    "            new_inp.append(1)\n",
    "        new_input_ids.append(torch.as_tensor(new_inp))\n",
    "    x_new = torch.nn.utils.rnn.pad_sequence(new_input_ids, batch_first=True, padding_value=pad_id)\n",
    "    x_new = x_new.to(input_ids.device)\n",
    "    return x_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487099ae-e30e-4589-a906-cadde423399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'ct5-small-en-wiki-pytorch'\n",
    "config_file = dirname+'/config.json'\n",
    "checkpoint_file = dirname+'/flax_model.msgpack'\n",
    "tokenizer_config_file = dirname+'/tokenizer_config.json'\n",
    "tokenizer_file = dirname+'/tokenizer.json'\n",
    "special_tokens_map_file = dirname+'/special_tokens_map.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca10aa32-5b3a-40be-bc13-8386aa8f44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 10:24:32.531462: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from modeling_ct5 import CT5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(dirname)\n",
    "model = CT5ForConditionalGeneration.from_pretrained(dirname)\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5bfce8d-597f-4a31-8061-a2d15adf04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model_t5 = model_t5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818a96d4-a688-428c-8736-582932110e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/mtreviso/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72a1c9e801d4c64ac79dec530d39d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29af96a-9879-483d-af21-ca914df84adf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_ex at 0x7fa04a86be50> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342176bfc7a043dd915d5bdd644e176c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afae865330e54914a165b4883aff4dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1449 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length, max_length = 480, 512\n",
    "\n",
    "def encode_ex(example):\n",
    "    return tokenizer(\n",
    "        example['text'], \n",
    "        return_attention_mask=False,\n",
    "        padding='max_length', \n",
    "        max_length=max_length,\n",
    "        truncation=True, \n",
    "    )\n",
    "\n",
    "c = 0\n",
    "def filter_ex(example):\n",
    "    return max_length*0.85 <= len(example[\"text\"].split()) <= max_length*1.15\n",
    "\n",
    "tokenized_dataset = train_dataset.filter(filter_ex).map(encode_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29acaab6-da59-465e-94e6-de799b336cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain_chunked_t5 import FlaxDataCollatorForT5MLM\n",
    "\n",
    "data_collator = FlaxDataCollatorForT5MLM(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_density=0.15,\n",
    "    mean_noise_span_length=3.0,\n",
    "    input_length=None,\n",
    "    target_length=None,\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "model_inputs = data_collator(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ec1123-5255-4862-b102-7c754c821717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(t):\n",
    "    return t.replace('<', ' <').replace('>', '> ').replace('  ', ' ').strip()\n",
    "\n",
    "input_texts = [sanitize(tokenizer.decode(x)) for x in model_inputs['input_ids']]\n",
    "label_texts = [sanitize(tokenizer.decode(x)) for x in model_inputs['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778cbd9a-a7bc-49db-be71-a1e90bb159ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Levenshtein import seqratio\n",
    "\n",
    "def break_by_chunks(ids):\n",
    "    m = (ids >= 32000) & (ids <= 32099)\n",
    "    return torch.tensor_split(ids, m.nonzero().squeeze().cpu())\n",
    "\n",
    "def chunk_accuracy(pred_ids, gold_ids, fuzzy=False):\n",
    "    # [1:] to remove decoder_start_id\n",
    "    chunked_pred_ids = break_by_chunks(pred_ids[1:])\n",
    "    chunked_gold_ids = break_by_chunks(gold_ids)\n",
    "    r = 0\n",
    "    n = 0\n",
    "    for pred, gold in zip(chunked_pred_ids, chunked_gold_ids):\n",
    "        if len(pred) <= 1 or len(gold) <= 1:\n",
    "            continue\n",
    "        # [1:] to remove the sentinel token\n",
    "        pred = list(map(str, pred[pred != 0].cpu().tolist()))[1:]\n",
    "        gold = list(map(str, gold[gold != 0].cpu().tolist()))[1:]\n",
    "        n += 1\n",
    "        if fuzzy:\n",
    "            \n",
    "            r += seqratio(pred, gold)\n",
    "        else:\n",
    "            r += float(pred == gold)\n",
    "    return r / n\n",
    "\n",
    "def compute_accuracy(input_batches, label_batches, gen_model='ct5'):\n",
    "    acc_random = []\n",
    "    acc_fuzzy = []\n",
    "    acc_match = []\n",
    "    model.cuda()\n",
    "    model_t5.cuda()\n",
    "    for input_ids, label_ids in tqdm(zip(input_batches, label_batches)):\n",
    "        if gen_model == 'ct5':\n",
    "            kw = dict(eoc_token_id=tokenizer.sep_token_id)\n",
    "            m = model\n",
    "        else:\n",
    "            kw = dict()\n",
    "            m = model_t5\n",
    "            \n",
    "        generated_ids = m.generate(\n",
    "            input_ids.cuda(), \n",
    "            attention_mask=input_ids.cuda() != 0,\n",
    "            use_cache=False,\n",
    "            do_sample=False,\n",
    "            max_length=512,\n",
    "            num_beams=1,\n",
    "            **kw\n",
    "        )\n",
    " \n",
    "        generated_ids = generated_ids.cpu()\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            ell = label_ids[i] >= 32000\n",
    "            random_labels = torch.randint(0, 32000, size=label_ids[i].shape)\n",
    "            random_labels[ell] = label_ids[i][ell]\n",
    "            acc_random.append(chunk_accuracy(generated_ids[i], random_labels, fuzzy=True))\n",
    "            acc_fuzzy.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=True))\n",
    "            acc_match.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=False))\n",
    "    \n",
    "    print('Acc match: {:.4f} ({:.4f})'.format(np.mean(acc_match), np.std(acc_match)))\n",
    "    print('Acc fuzzy: {:.4f} ({:.4f})'.format(np.mean(acc_fuzzy), np.std(acc_fuzzy)))\n",
    "    print('Acc random: {:.4f} ({:.4f})'.format(np.mean(acc_random), np.std(acc_random)))\n",
    "\n",
    "def masked_perplexity(log_probas, mask, reduce='mean'):\n",
    "    num = torch.sum(log_probas * mask.float(), dim=-1)\n",
    "    div = mask.sum(-1).float()\n",
    "    perpl = torch.exp(-num/div)\n",
    "    if reduce == 'mean':\n",
    "        return perpl.mean().item()\n",
    "    return perpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff747e-3777-434b-b826-a05fcc7b3fe8",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edab07b-63e8-4888-99d7-330f0676ed12",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "with profiler.profile(profile_memory=True, use_cuda=True, with_flops=True) as prof:\n",
    "    generated_ids = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=input_ids != 0,\n",
    "        eoc_token_id=tokenizer.sep_token_id,\n",
    "        use_cache=False,\n",
    "        do_sample=False,\n",
    "        max_length=512,\n",
    "        num_beams=1,\n",
    "    )\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a499e-613a-4354-9d52-4199a248e4b9",
   "metadata": {},
   "source": [
    "## Timing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5d6f1-afbf-4345-98b8-c078d6f1b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_chunked_t5(input_ids):\n",
    "    _ = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=input_ids != 0,\n",
    "        eoc_token_id=tokenizer.sep_token_id,\n",
    "        use_cache=False,\n",
    "        do_sample=False,\n",
    "        max_length=512,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    \n",
    "def generation_regular_t5(input_ids):\n",
    "    _ = model_t5.generate(\n",
    "        input_ids, \n",
    "        attention_mask=input_ids != 0,\n",
    "        use_cache=False,\n",
    "        do_sample=False,\n",
    "        max_length=512,\n",
    "        num_beams=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d01cdb-472d-40e0-accd-864e5fdef4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "from itertools import product\n",
    "\n",
    "# sequence_lengths = [64, 128, 512, 1024]\n",
    "sequence_lengths = [512]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_threads = 1\n",
    "results = []\n",
    "\n",
    "for batch_size, seq_len in tqdm(product(batch_sizes, sequence_lengths)):\n",
    "    label = 'serial vs parallel greedy search'\n",
    "    sub_label = f'[{batch_size}, {seq_len}]'\n",
    "    input_ids = tokenizer(input_samples[:batch_size], return_tensors=\"pt\", padding=True).input_ids\n",
    "    input_ids = input_ids.cuda()\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='generation_chunked_t5(input_ids)',\n",
    "        setup='from __main__ import generation_chunked_t5',\n",
    "        globals={'input_ids': input_ids},\n",
    "        num_threads=num_threads,\n",
    "        label=label,\n",
    "        sub_label=sub_label,\n",
    "        description='chunked t5',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='generation_regular_t5(input_ids)',\n",
    "        setup='from __main__ import generation_regular_t5',\n",
    "        globals={'input_ids': input_ids},\n",
    "        num_threads=num_threads,\n",
    "        label=label,\n",
    "        sub_label=sub_label,\n",
    "        description='regular t5',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0bf51a-3d96-440c-816e-320282748d0b",
   "metadata": {},
   "source": [
    "## Example\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6119f7fd-75b1-457e-8cb0-14d522be9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 32099, 10681, 16, 32098, 2447, 1, 0, 0, 0, 0, 0]\n",
      "[4417, 5116, 845, 132, 19, 150, 194, 12, 32099, 16, 11380, 1]\n",
      "[499, 629, 19, 32099, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The <extra_id_0> walks in <extra_id_1> park\",\n",
    "    \"UN Chief says there is no way to <extra_id_0> in Syria\",\n",
    "    \"My house is <extra_id_0>\",\n",
    "    # \"<extra_id_0> is cool\",\n",
    "]\n",
    "\n",
    "input_ids = tokenizer(texts, return_tensors=\"pt\", padding=True).input_ids\n",
    "input_mask = (input_ids >= 32000) & (input_ids <= 32099)\n",
    "print(input_ids[0].tolist())\n",
    "print(input_ids[1].tolist())\n",
    "print(input_ids[2].tolist())\n",
    "# print(input_ids[3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9b784f2-330a-4c36-8f5c-dbca9cceab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Walking Trail walks in the park\n",
      "UN Chief says there is no way to treat Syria in Syria\n",
      "My house is My house is My house\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=input_ids != 0,\n",
    "    use_cache=False,\n",
    "    do_sample=False,\n",
    "    top_p=0.95,\n",
    "    top_k=30,\n",
    "    num_beams=1,\n",
    "    eoc_token_id=tokenizer.sep_token_id,\n",
    "    max_chunk_size=5,\n",
    ")\n",
    "merged_ids = merge_input_and_gen_ids(input_ids, generated_ids)\n",
    "for i in range(len(merged_ids)):\n",
    "    print(tokenizer.decode(merged_ids[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "708ef306-4b9e-43c2-99c7-46aeae0c1d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<extra_id_0>', '▁Walking', '▁Trail', '</c>', '<extra_id_1>', '▁the', '</c>', '<extra_id_2>', '</s>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<extra_id_0>', '▁treat', '▁Syria', '</c>', '<extra_id_1>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<extra_id_0>', '▁My', '▁house', '▁is', '▁My', '▁house', '<pad>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(generated_ids)):\n",
    "    print(tokenizer.convert_ids_to_tokens(generated_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ec78ea8-87a4-476b-8e89-11a2a34be7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._get_decoder_attention_mask_from_input_ids(generated_ids).long()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3563fb46-28c4-47d9-b188-4683309b9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=input_ids != 0,\n",
    "    use_cache=False,\n",
    "    do_sample=False,\n",
    "    top_p=0.95,\n",
    "    top_k=30,\n",
    "    num_beams=1,\n",
    "    eoc_token_id=tokenizer.sep_token_id,\n",
    "    max_chunk_size=5,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "generated_ids = out['sequences']\n",
    "\n",
    "log_probas = torch.nn.utils.rnn.pad_sequence(out['scores'], batch_first=True).log_softmax(dim=-1).max(dim=-1)[0]\n",
    "mask = torch.nn.utils.rnn.pad_sequence([torch.ones(len(out['scores'][i])) \n",
    "                                        for i in range(len(out['scores']))], batch_first=True)\n",
    "masked_perplexity(log_probas, mask.long().bool(), reduce=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96e1f77a-fd4b-4636-ae5a-fc7a7f681bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hague walks in the park park\n",
      "UN Chief says there is no way to say in Syria. Syria in Syria\n",
      "My house is my house. My house in My house is my house. My house My house has\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model_t5.generate(\n",
    "    input_ids, \n",
    "    attention_mask=input_ids != 0,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=30,\n",
    "    num_beams=1,\n",
    ")\n",
    "merged_ids = merge_input_and_gen_ids(input_ids, generated_ids)\n",
    "for i in range(len(merged_ids)):\n",
    "    print(tokenizer.decode(merged_ids[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551826a-8a14-4bf9-ae8a-8648bdd73141",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c2e31e46-5d7f-4b5f-8268-10a93bc5a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(input_batches, label_batches, gen_model, gen_kwargs={}):\n",
    "    acc_random = []\n",
    "    acc_fuzzy = []\n",
    "    acc_match = []\n",
    "    perpls = []\n",
    "    if gen_model == 'ct5':\n",
    "        m = model\n",
    "        gen_kwargs['eoc_token_id'] = tokenizer.sep_token_id\n",
    "    else:\n",
    "        m = model_t5\n",
    "    \n",
    "    for input_ids, label_ids in tqdm(zip(input_batches, label_batches)):\n",
    "        \n",
    "        generated_ids = m.generate(\n",
    "            input_ids.cuda(), \n",
    "            attention_mask=input_ids.cuda() != 0,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        if gen_model == 'ct5':\n",
    "            outputs = m(\n",
    "                input_ids=input_ids.cuda(), \n",
    "                attention_mask=input_ids.cuda() != 0,\n",
    "                decoder_input_ids=generated_ids.cuda(),\n",
    "                decoder_attention_mask=m._get_decoder_attention_mask_from_input_ids(generated_ids.cuda()),\n",
    "                # labels=label_ids\n",
    "            )\n",
    "            log_probas = outputs.logits.log_softmax(dim=-1)\n",
    "            fix_mask = ((generated_ids.cuda() >= 32000) & (generated_ids.cuda() <= 32099)) | (generated_ids.cuda() <= 1)\n",
    "            fix_mask = (~fix_mask).bool()\n",
    "            lens = fix_mask.sum(-1)\n",
    "            slices = lens.cumsum(-1).cpu()\n",
    "            log_probas = log_probas[fix_mask].tensor_split(slices)[:-1]\n",
    "            log_probas = torch.nn.utils.rnn.pad_sequence(log_probas, batch_first=True)\n",
    "            gen_ids = generated_ids.roll(-1, dims=-1)[fix_mask].tensor_split(slices)[:-1]\n",
    "            gen_ids = torch.nn.utils.rnn.pad_sequence(gen_ids, batch_first=True).long()\n",
    "            log_probas = log_probas.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            mask = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in lens], batch_first=True)\n",
    "            mask = mask.long().bool()\n",
    "        else:\n",
    "            outputs = m(\n",
    "                input_ids=input_ids.cuda(), \n",
    "                attention_mask=input_ids.cuda() != 0,\n",
    "                decoder_input_ids=generated_ids.cuda(),\n",
    "                decoder_attention_mask=generated_ids.cuda() != 0,\n",
    "                # labels=label_ids\n",
    "            )\n",
    "            log_probas = outputs.logits.log_softmax(dim=-1)\n",
    "            fix_mask = ((generated_ids.cuda() >= 32000) & (generated_ids.cuda() <= 32099)) | (generated_ids.cuda() <= 1)\n",
    "            fix_mask = (~fix_mask).bool()\n",
    "            lens = fix_mask.sum(-1)\n",
    "            slices = lens.cumsum(-1).cpu()\n",
    "            log_probas = log_probas[fix_mask].tensor_split(slices)[:-1]\n",
    "            log_probas = torch.nn.utils.rnn.pad_sequence(log_probas, batch_first=True)\n",
    "            gen_ids = generated_ids.roll(-1, dims=-1)[fix_mask].tensor_split(slices)[:-1]\n",
    "            gen_ids = torch.nn.utils.rnn.pad_sequence(gen_ids, batch_first=True).long()\n",
    "            log_probas = log_probas.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            mask = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in lens], batch_first=True)\n",
    "            mask = mask.long().bool()\n",
    "        perpl = masked_perplexity(log_probas.cuda(), mask.cuda(), reduce=None)\n",
    "        perpls.extend(perpl.cpu().tolist())\n",
    "        \n",
    "        generated_ids = generated_ids.cpu()\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            ell = label_ids[i] >= 32000\n",
    "            random_labels = torch.randint(0, 32000, size=label_ids[i].shape)\n",
    "            random_labels[ell] = label_ids[i][ell]\n",
    "            acc_random.append(chunk_accuracy(generated_ids[i], random_labels, fuzzy=True))\n",
    "            acc_fuzzy.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=True))\n",
    "            acc_match.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=False))\n",
    "    \n",
    "    print(gen_model)\n",
    "    print('Acc match: {:.4f} ({:.4f})'.format(np.mean(acc_match), np.std(acc_match)))\n",
    "    print('Acc fuzzy: {:.4f} ({:.4f})'.format(np.mean(acc_fuzzy), np.std(acc_fuzzy)))\n",
    "    print('Acc random: {:.4f} ({:.4f})'.format(np.mean(acc_random), np.std(acc_random)))\n",
    "    print('Perplexity: {:.4f} ({:.4f})'.format(np.mean(perpls), np.std(perpls)))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "525090b2-ac7c-49d4-8856-881b824fbd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b988846950e4c2e87763107a47bf9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d5a2a48cb34608ab158aef53ba29b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607\n",
      "1607\n",
      "218\n",
      "371\n",
      "96\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cuda()\n",
    "model_t5 = model_t5.eval().cuda()\n",
    "batch_size = 16\n",
    "min_len, max_len = 128, 512\n",
    "\n",
    "def filter_ex(example):\n",
    "    return 512*0.85 <= len(example[\"text\"].split())\n",
    "\n",
    "tokenized_dataset = train_dataset.filter(filter_ex).map(encode_ex)\n",
    "data_collator = FlaxDataCollatorForT5MLM(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_density=0.15,\n",
    "    mean_noise_span_length=3.0,\n",
    "    input_length=None,\n",
    "    target_length=None,\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "model_inputs = data_collator(tokenized_dataset)\n",
    "input_samples = [sanitize(tokenizer.decode(x)).replace('</s>', '').strip() for x in model_inputs['input_ids']]\n",
    "label_samples = [sanitize(tokenizer.decode(x)).replace('</s>', '').strip() for x in model_inputs['labels']]\n",
    "\n",
    "print(len(input_samples))\n",
    "print(len(label_samples))\n",
    "print(min(map(lambda x: len(x.split()), input_samples)))\n",
    "print(max(map(lambda x: len(x.split()), input_samples)))\n",
    "print(min(map(lambda x: len(x.split()), label_samples)))\n",
    "print(max(map(lambda x: len(x.split()), label_samples)))\n",
    "\n",
    "input_batches = [\n",
    "    tokenizer(input_samples[i:i+batch_size], return_tensors=\"pt\", padding=True).input_ids\n",
    "    for i in range(0, len(input_samples), batch_size)\n",
    "]\n",
    "label_batches = [\n",
    "    tokenizer(label_samples[i:i+batch_size], return_tensors=\"pt\", padding=True).input_ids\n",
    "    for i in range(0, len(label_samples), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ea45148b-569e-415b-9b39-fcd5a07c6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:21,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct5\n",
      "Acc match: 0.0591 (0.0485)\n",
      "Acc fuzzy: 0.5753 (0.0325)\n",
      "Acc random: 0.4675 (0.0166)\n",
      "Perplexity: 2.5666 (0.8016)\n",
      "Elapsed time: 21.91607860568911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:58,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5\n",
      "Acc match: 0.1025 (0.0596)\n",
      "Acc fuzzy: 0.5610 (0.0374)\n",
      "Acc random: 0.4720 (0.0206)\n",
      "Perplexity: 3.3864 (0.5265)\n",
      "Elapsed time: 118.01890839915723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    num_beams=1,\n",
    "    max_chunk_size=5,\n",
    "    use_cache=False\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='ct5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    # num_beams=1,\n",
    "    # max_chunk_size=5,\n",
    "    use_cache=True\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='t5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "daea8a5e-4f25-4ec9-acab-f9ce25b7cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:21,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct5\n",
      "Acc match: 0.0591 (0.0485)\n",
      "Acc fuzzy: 0.5753 (0.0325)\n",
      "Acc random: 0.4679 (0.0159)\n",
      "Perplexity: 2.5666 (0.8016)\n",
      "Elapsed time: 21.852749910205603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:57,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5\n",
      "Acc match: 0.1025 (0.0596)\n",
      "Acc fuzzy: 0.5610 (0.0374)\n",
      "Acc random: 0.4726 (0.0203)\n",
      "Perplexity: 3.3864 (0.5265)\n",
      "Elapsed time: 117.84094660636038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    num_beams=1,\n",
    "    max_chunk_size=5,\n",
    "    use_cache=False\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='ct5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    # num_beams=1,\n",
    "    # max_chunk_size=5,\n",
    "    use_cache=True\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='t5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4615d-c753-412a-bec3-d84e971547ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
