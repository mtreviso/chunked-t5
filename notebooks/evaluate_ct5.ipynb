{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714f006-b7ca-4edf-9f4f-959a95dd102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fbe9f-4e4f-4526-a0bb-f74284f5824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7b466-24f7-401f-b2e7-1b62190ba133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils import merge_input_and_gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10aa32-5b3a-40be-bc13-8386aa8f44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from modeling_ct5 import CT5ForConditionalGeneration\n",
    "\n",
    "dirname = 'ct5-small-en-wiki-pytorch'\n",
    "tokenizer = AutoTokenizer.from_pretrained(dirname)\n",
    "model_ct5 = CT5ForConditionalGeneration.from_pretrained(dirname)\n",
    "model_ct5 = model_ct5.eval().cuda()\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(dirname)\n",
    "model_t5 = model_t5.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a96d4-a688-428c-8736-582932110e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pretrain_chunked_t5 import FlaxDataCollatorForT5MLM\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9e11f-3382-434a-9a2d-d7758c977454",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length, max_length = 480, 512\n",
    "batch_size = 16\n",
    "\n",
    "def encode_ex(example):\n",
    "    return tokenizer(\n",
    "        example['text'], \n",
    "        return_attention_mask=False,\n",
    "        padding='max_length', \n",
    "        max_length=max_length,\n",
    "        truncation=True, \n",
    "    )\n",
    "\n",
    "def filter_ex(example):\n",
    "    return min_length <= len(example[\"text\"].split())\n",
    "\n",
    "def sanitize(t):\n",
    "    return t.replace('<', ' <').replace('>', '> ').replace('  ', ' ').strip()\n",
    "\n",
    "tokenized_dataset = train_dataset.filter(filter_ex).map(encode_ex)\n",
    "data_collator = FlaxDataCollatorForT5MLM(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_density=0.15,\n",
    "    mean_noise_span_length=3.0,\n",
    "    input_length=None,\n",
    "    target_length=None,\n",
    "    pad_token_id=model_ct5.config.pad_token_id,\n",
    "    decoder_start_token_id=model_ct5.config.decoder_start_token_id,\n",
    ")\n",
    "model_inputs = data_collator(tokenized_dataset)\n",
    "input_samples = [sanitize(tokenizer.decode(x)).replace('</s>', '').strip() for x in model_inputs['input_ids']]\n",
    "label_samples = [sanitize(tokenizer.decode(x)).replace('</s>', '').strip() for x in model_inputs['labels']]\n",
    "\n",
    "print(len(input_samples))\n",
    "print(min(map(lambda x: len(x.split()), input_samples)))\n",
    "print(max(map(lambda x: len(x.split()), input_samples)))\n",
    "print(len(label_samples))\n",
    "print(min(map(lambda x: len(x.split()), label_samples)))\n",
    "print(max(map(lambda x: len(x.split()), label_samples)))\n",
    "\n",
    "input_batches = [\n",
    "    tokenizer(input_samples[i:i+batch_size], return_tensors=\"pt\", padding=True).input_ids\n",
    "    for i in range(0, len(input_samples), batch_size)\n",
    "]\n",
    "label_batches = [\n",
    "    tokenizer(label_samples[i:i+batch_size], return_tensors=\"pt\", padding=True).input_ids\n",
    "    for i in range(0, len(label_samples), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551826a-8a14-4bf9-ae8a-8648bdd73141",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "778cbd9a-a7bc-49db-be71-a1e90bb159ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Levenshtein import seqratio\n",
    "\n",
    "def break_by_chunks(ids):\n",
    "    m = (ids >= 32000) & (ids <= 32099)\n",
    "    return torch.tensor_split(ids, m.nonzero().squeeze().cpu())\n",
    "\n",
    "def chunk_accuracy(pred_ids, gold_ids, fuzzy=False):\n",
    "    # [1:] to remove decoder_start_id\n",
    "    chunked_pred_ids = break_by_chunks(pred_ids[1:])\n",
    "    chunked_gold_ids = break_by_chunks(gold_ids)\n",
    "    r = 0\n",
    "    n = 0\n",
    "    for pred, gold in zip(chunked_pred_ids, chunked_gold_ids):\n",
    "        if len(pred) <= 1 or len(gold) <= 1:\n",
    "            continue\n",
    "        # [1:] to remove the sentinel token\n",
    "        pred = list(map(str, pred[pred != 0].cpu().tolist()))[1:]\n",
    "        gold = list(map(str, gold[gold != 0].cpu().tolist()))[1:]\n",
    "        n += 1\n",
    "        if fuzzy:\n",
    "            \n",
    "            r += seqratio(pred, gold)\n",
    "        else:\n",
    "            r += float(pred == gold)\n",
    "    return r / n\n",
    "\n",
    "def compute_accuracy(input_batches, label_batches, gen_model='ct5'):\n",
    "    acc_random = []\n",
    "    acc_fuzzy = []\n",
    "    acc_match = []\n",
    "    model.cuda()\n",
    "    model_t5.cuda()\n",
    "    for input_ids, label_ids in tqdm(zip(input_batches, label_batches)):\n",
    "        if gen_model == 'ct5':\n",
    "            kw = dict(eoc_token_id=tokenizer.sep_token_id)\n",
    "            m = model\n",
    "        else:\n",
    "            kw = dict()\n",
    "            m = model_t5\n",
    "            \n",
    "        generated_ids = m.generate(\n",
    "            input_ids.cuda(), \n",
    "            attention_mask=input_ids.cuda() != 0,\n",
    "            use_cache=False,\n",
    "            do_sample=False,\n",
    "            max_length=512,\n",
    "            num_beams=1,\n",
    "            **kw\n",
    "        )\n",
    " \n",
    "        generated_ids = generated_ids.cpu()\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            ell = label_ids[i] >= 32000\n",
    "            random_labels = torch.randint(0, 32000, size=label_ids[i].shape)\n",
    "            random_labels[ell] = label_ids[i][ell]\n",
    "            acc_random.append(chunk_accuracy(generated_ids[i], random_labels, fuzzy=True))\n",
    "            acc_fuzzy.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=True))\n",
    "            acc_match.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=False))\n",
    "    \n",
    "    print('Acc match: {:.4f} ({:.4f})'.format(np.mean(acc_match), np.std(acc_match)))\n",
    "    print('Acc fuzzy: {:.4f} ({:.4f})'.format(np.mean(acc_fuzzy), np.std(acc_fuzzy)))\n",
    "    print('Acc random: {:.4f} ({:.4f})'.format(np.mean(acc_random), np.std(acc_random)))\n",
    "\n",
    "def masked_perplexity(log_probas, mask, reduce='mean'):\n",
    "    num = torch.sum(log_probas * mask.float(), dim=-1)\n",
    "    div = mask.sum(-1).float()\n",
    "    perpl = torch.exp(-num/div)\n",
    "    if reduce == 'mean':\n",
    "        return perpl.mean().item()\n",
    "    return perpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e31e46-5d7f-4b5f-8268-10a93bc5a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(input_batches, label_batches, gen_model, gen_kwargs={}):\n",
    "    acc_random = []\n",
    "    acc_fuzzy = []\n",
    "    acc_match = []\n",
    "    perpls = []\n",
    "    m = model_ct5 if gen_model == 'ct5' else model_t5\n",
    "    \n",
    "    for input_ids, label_ids in tqdm(zip(input_batches, label_batches)):\n",
    "        input_ids = input_ids.cuda()\n",
    "        label_ids = label_ids.cuda()\n",
    "        \n",
    "        generated_ids = m.generate(\n",
    "            input_ids, \n",
    "            attention_mask=input_ids != 0,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        generated_ids = generated_ids.cuda()\n",
    "        \n",
    "        if gen_model == 'ct5':\n",
    "            outputs = m(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=input_ids != 0,\n",
    "                decoder_input_ids=generated_ids,\n",
    "                decoder_attention_mask=m._get_decoder_attention_mask_from_input_ids(generated_ids),\n",
    "                # labels=label_ids\n",
    "            )\n",
    "            log_probas = outputs.logits.log_softmax(dim=-1)\n",
    "            fix_mask = ((generated_ids >= 32000) & (generated_ids <= 32099)) | (generated_ids <= 1)\n",
    "            fix_mask = (~fix_mask).bool()\n",
    "            lens = fix_mask.sum(-1)\n",
    "            slices = lens.cumsum(-1).cpu()\n",
    "            log_probas = log_probas[fix_mask].tensor_split(slices)[:-1]\n",
    "            log_probas = torch.nn.utils.rnn.pad_sequence(log_probas, batch_first=True)\n",
    "            gen_ids = generated_ids.roll(-1, dims=-1)[fix_mask].tensor_split(slices)[:-1]\n",
    "            gen_ids = torch.nn.utils.rnn.pad_sequence(gen_ids, batch_first=True).long()\n",
    "            log_probas = log_probas.gather(2, gen_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            mask = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in lens], batch_first=True)\n",
    "            mask = mask.long().bool().cuda()\n",
    "        else:\n",
    "            outputs = m(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=input_ids != 0,\n",
    "                decoder_input_ids=generated_ids,\n",
    "                decoder_attention_mask=generated_ids != 0,\n",
    "                # labels=label_ids\n",
    "            )\n",
    "            log_probas = outputs.logits.log_softmax(dim=-1)\n",
    "            log_probas = log_probas.gather(2, generated_ids.roll(-1, dims=-1).unsqueeze(-1)).squeeze(-1)\n",
    "            mask = generated_ids != 0\n",
    "        perpl = masked_perplexity(log_probas.cuda(), mask.cuda(), reduce=None)\n",
    "        perpls.extend(perpl.cpu().tolist())\n",
    "        \n",
    "        generated_ids = generated_ids.cpu()\n",
    "        label_ids = label_ids.cpu()\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            ell = label_ids[i] >= 32000\n",
    "            random_labels = torch.randint(0, 32000, size=label_ids[i].shape)\n",
    "            random_labels[ell] = label_ids[i][ell]\n",
    "            acc_random.append(chunk_accuracy(generated_ids[i], random_labels, fuzzy=True))\n",
    "            acc_fuzzy.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=True))\n",
    "            acc_match.append(chunk_accuracy(generated_ids[i], label_ids[i], fuzzy=False))\n",
    "    \n",
    "    print(gen_model)\n",
    "    print('Acc match: {:.4f} ({:.4f})'.format(np.mean(acc_match), np.std(acc_match)))\n",
    "    print('Acc fuzzy: {:.4f} ({:.4f})'.format(np.mean(acc_fuzzy), np.std(acc_fuzzy)))\n",
    "    print('Acc random: {:.4f} ({:.4f})'.format(np.mean(acc_random), np.std(acc_random)))\n",
    "    print('Perplexity: {:.4f} ({:.4f})'.format(np.mean(perpls), np.std(perpls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea45148b-569e-415b-9b39-fcd5a07c6604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:10,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct5\n",
      "Acc match: 0.0896 (0.0549)\n",
      "Acc fuzzy: 0.5750 (0.0324)\n",
      "Acc random: 0.4766 (0.0157)\n",
      "Perplexity: 1.5445 (0.3301)\n",
      "Elapsed time: 10.685760903172195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:58,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5\n",
      "Acc match: 0.1034 (0.0591)\n",
      "Acc fuzzy: 0.5737 (0.0392)\n",
      "Acc random: 0.4685 (0.0208)\n",
      "Perplexity: 3.3637 (0.6629)\n",
      "Elapsed time: 118.43209824990481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    num_beams=1,\n",
    "    use_cache=False,\n",
    "    eoc_token_id=tokenizer.vocab['</c>'],\n",
    "    max_chunk_size=5\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='ct5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    top_k=None,\n",
    "    top_p=0.95,\n",
    "    max_length=512,\n",
    "    num_beams=1,\n",
    "    use_cache=True\n",
    "    # eoc_token_id=tokenizer.vocab['</c>'],\n",
    "    # max_chunk_size=5\n",
    ")\n",
    "torch.cuda.empty_cache()  # clear cache before timing\n",
    "torch.cuda.synchronize(0)  # wait for initialization to finish\n",
    "time1 = time.perf_counter()\n",
    "evaluate_model(input_batches, label_batches, gen_model='t5', gen_kwargs=gen_kwargs)\n",
    "torch.cuda.synchronize(0)\n",
    "time2 = time.perf_counter()\n",
    "print('Elapsed time: {}'.format(time2 - time1))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4615d-c753-412a-bec3-d84e971547ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
